{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOahHsPQj7jxk2Stv9msHlI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhanush20022/Deep-Learning/blob/main/dos_Cats_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# NEW: mount Google Drive\n",
        "# -----------------------------\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ---- EDIT this if your folder is in a different Drive location ----\n",
        "# Put your Drive folder name where you placed cat_dog (exactly as in Drive)\n",
        "# e.g. if you uploaded folder to MyDrive root as \"cat_dog\", keep as below\n",
        "# Explicitly set dataset_root to the correct Google Drive path\n",
        "# >>> CHANGE this string to the correct path in your Drive if needed:\n",
        "dataset_root = \"/content/drive/MyDrive/Datasets_of_ML/cat_dog\"\n",
        "\n",
        "# Normalize path\n",
        "dataset_root = os.path.abspath(dataset_root)\n",
        "train_folder = os.path.join(dataset_root, \"training_set\")\n",
        "test_folder  = os.path.join(dataset_root, \"test_set\")\n",
        "single_pred_folder = os.path.join(dataset_root, \"single_prediction\")\n",
        "\n",
        "\n",
        "print(\"Using dataset root:\", dataset_root)\n",
        "print(\"Training folder:\", train_folder)\n",
        "print(\"Test folder:\", test_folder)\n",
        "print(\"Single-pred folder (optional):\", single_pred_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rfK6ra0PVWck",
        "outputId": "79dfd1a7-fff3-4dd1-ccdb-52a03b01f7ee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Using dataset root: /content/drive/MyDrive/Datasets_of_ML/cat_dog\n",
            "Training folder: /content/drive/MyDrive/Datasets_of_ML/cat_dog/training_set\n",
            "Test folder: /content/drive/MyDrive/Datasets_of_ML/cat_dog/test_set\n",
            "Single-pred folder (optional): /content/drive/MyDrive/Datasets_of_ML/cat_dog/single_prediction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let us now see what each of the above packages are imported for :**\n",
        "\n",
        "**In line 1**, we’ve imported Sequential from keras.models, to initialise our neural network model as a sequential network. There are two basic ways of initialising a neural network, either by a sequence of layers or as a graph.\n",
        "\n",
        "**In line 2**, we’ve imported Conv2D from keras.layers, this is to perform the convolution operation i.e the first step of a CNN, on the training images. Since we are working on images here, which a basically 2 Dimensional arrays, we’re using Convolution 2-D, you may have to use Convolution 3-D while dealing with videos, where the third dimension will be time.\n",
        "\n",
        "**In line 3**, we’ve imported MaxPooling2D from keras.layers, which is used for pooling operation, that is the step — 2 in the process of building a cnn. For building this particular neural network, we are using a Maxpooling function, there exist different types of pooling operations like Min Pooling, Mean Pooling, etc. Here in MaxPooling we need the maximum value pixel from the respective region of interest.\n",
        "\n",
        "**In line 4**, we’ve imported Flatten from keras.layers, which is used for Flattening. Flattening is the process of converting all the resultant 2 dimensional arrays into a single long continuous linear vector.\n",
        "\n",
        "**And finally in line 5**, we’ve imported Dense from keras.layers, which is used to perform the full connection of the neural network, which is the step 4 in the process of building a CNN."
      ],
      "metadata": {
        "id": "fBsjZDxQHZWZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9jaA5ftKHE5S"
      },
      "outputs": [],
      "source": [
        "# Importing the Keras libraries and packages\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now, we will create an object of the sequential class below:**"
      ],
      "metadata": {
        "id": "_QNqvyDcIFe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Part-1\n",
        "classifier = Sequential()"
      ],
      "metadata": {
        "id": "x3ZPkx4tHXTb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let us now code the Convolution step, you will be surprised to see how easy it is to actually implement these complex operations in a single line of code in python, thanks to Keras.**"
      ],
      "metadata": {
        "id": "5A0FedT0IIxT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let’s break down the above code function by function. We took the object which already has an idea of how our neural network is going to be(Sequential), then we added a convolution layer by using the “Conv2D” function. The Conv2D function is taking 4 arguments, the first is the number of filters i.e 32 here, the second argument is the shape each filter is going to be i.e 3x3 here, the third is the input shape and the type of image(RGB or Black and White)of each image i.e the input image our CNN is going to be taking is of a 64x64 resolution and “3” stands for RGB, which is a colour img, the fourth argument is the activation function we want to use, here ‘relu’ stands for a rectifier function.**"
      ],
      "metadata": {
        "id": "UrwARDuVWRmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1 - Convolution\n",
        "classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))"
      ],
      "metadata": {
        "id": "jEcxmECWWk8l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc337545-ab41-4e8c-8afa-9a10a9a5f497"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We start by taking our classifier object and add the pooling layer. We take a 2x2 matrix we’ll have minimum pixel loss and get a precise region where the feature are located. Again, to understand the actual math behind Pooling, i suggest you to go learn from an external source, this tutorial concentrates more on the implementation part. We just reduced the complexity of the model without reducing it’s performance.**"
      ],
      "metadata": {
        "id": "jM2HY44IXB5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2 - Pooling\n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2)))"
      ],
      "metadata": {
        "id": "GMUaUSHLWk3t"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding a second convolutional layer\n",
        "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2)))"
      ],
      "metadata": {
        "id": "PbiYMO7BWk0l"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**It’s time for us to now convert all the pooled images into a continuous vector through Flattening. Flattening is a very important step to understand. What we are basically doing here is taking the 2-D array, i.e pooled image pixels and converting them to a one dimensional single vector.**"
      ],
      "metadata": {
        "id": "IaNMKKZnXLaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3 - Flattening\n",
        "classifier.add(Flatten())"
      ],
      "metadata": {
        "id": "mWMdi4DzWksG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The above code is pretty self-explanatory. We’ve used flatten function to perform flattening, we no need to add any special parameters, keras will understand that the “classifier” object is already holding pooled image pixels and they need to be flattened.**"
      ],
      "metadata": {
        "id": "ImiNrk-tYZ5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4 - Full connection\n",
        "classifier.add(Dense(units = 128, activation = 'relu'))\n"
      ],
      "metadata": {
        "id": "Ac5Ub2auW0CL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**As you can see, Dense is the function to add a fully connected layer, ‘units’ is where we define the number of nodes that should be present in this hidden layer, these units value will be always between the number of input nodes and the output nodes but the art of choosing the most optimal number of nodes can be achieved only through experimental tries. Though it’s a common practice to use a power of 2. And the activation function will be a rectifier function.**"
      ],
      "metadata": {
        "id": "HdRemYesYnB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.add(Dense(units = 1, activation = 'sigmoid'))"
      ],
      "metadata": {
        "id": "ixJf6owlYrp1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now it’s time to initialise our output layer, which should contain only one node, as it is binary classification. This single node will give us a binary output of either a Cat or Dog.**"
      ],
      "metadata": {
        "id": "WeFCCLhzY2wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the CNN\n",
        "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2npeCpUOQKF-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**You can observe that the final layer contains only one node, and we will be using a sigmoid activation function for the final layer,\n",
        "Now that we have completed building our CNN model, it’s time to compile it.**"
      ],
      "metadata": {
        "id": "G5yltHhQaDZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above :\n",
        "\n",
        "* Optimizer parameter is to choose the stochastic gradient descent algorithm\n",
        "\n",
        "* Loss parameter is to choose the loss function.\n",
        "Finally, the metrics parameter is to choose the performance metric."
      ],
      "metadata": {
        "id": "mCnEqKEhaqfB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**It’s time to fit our CNN to the image dataset that you’ve downloaded.But before we do that, we are going to pre-process the images to prevent over-fitting. Overfitting is when you get a great training accuracy and very poor test accuracy due to overfitting of nodes from one layer to another.**"
      ],
      "metadata": {
        "id": "UfuX4hmmcPK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 2 - Fitting the CNN to the images\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Re-defining these for self-contained execution within this cell\n",
        "import os\n",
        "# Ensure dataset_root is correctly set, assuming it's already mounted\n",
        "dataset_root = \"/content/drive/MyDrive/Datasets_of_ML/cat_dog\"\n",
        "train_folder = os.path.join(dataset_root, \"training_set\")\n",
        "test_folder  = os.path.join(dataset_root, \"test_set\")\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "# <-- uses train_folder and test_folder resolved from Drive -->\n",
        "training_set = train_datagen.flow_from_directory(train_folder,\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'binary')\n",
        "test_set = test_datagen.flow_from_directory(test_folder,\n",
        "                                            target_size = (64, 64),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'binary')\n",
        "\n",
        "classifier.fit(training_set,\n",
        "                         steps_per_epoch = 8000,\n",
        "                         epochs = 25, # Increased epochs for retraining\n",
        "                         validation_data = test_set,\n",
        "                         validation_steps = 2000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fcbee5e-b349-4336-d120-4fc0778638b2",
        "id": "MH0FRu0LdjmI"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8005 images belonging to 2 classes.\n",
            "Found 2023 images belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m 251/8000\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31:17\u001b[0m 242ms/step - accuracy: 0.5526 - loss: 0.6811"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:116: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 9ms/step - accuracy: 0.5967 - loss: 0.6603 - val_accuracy: 0.6065 - val_loss: 0.6490\n",
            "Epoch 2/25\n",
            "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 9ms/step - accuracy: 0.6781 - loss: 0.6001 - val_accuracy: 0.7039 - val_loss: 0.5791\n",
            "Epoch 3/25\n",
            "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 9ms/step - accuracy: 0.7104 - loss: 0.5580 - val_accuracy: 0.7133 - val_loss: 0.5636\n",
            "Epoch 4/25\n",
            "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 9ms/step - accuracy: 0.7270 - loss: 0.5380 - val_accuracy: 0.7430 - val_loss: 0.5336\n",
            "Epoch 5/25\n",
            "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 9ms/step - accuracy: 0.7512 - loss: 0.5100 - val_accuracy: 0.7380 - val_loss: 0.5590\n",
            "Epoch 6/25\n",
            "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 9ms/step - accuracy: 0.7626 - loss: 0.4885 - val_accuracy: 0.7677 - val_loss: 0.4909\n",
            "Epoch 7/25\n",
            "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 9ms/step - accuracy: 0.7668 - loss: 0.4732 - val_accuracy: 0.7741 - val_loss: 0.4826\n",
            "Epoch 8/25\n",
            "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 9ms/step - accuracy: 0.7835 - loss: 0.4555 - val_accuracy: 0.7889 - val_loss: 0.4655\n",
            "Epoch 9/25\n",
            "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 9ms/step - accuracy: 0.7933 - loss: 0.4428 - val_accuracy: 0.7751 - val_loss: 0.4854\n",
            "Epoch 10/25\n",
            "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 9ms/step - accuracy: 0.7991 - loss: 0.4240 - val_accuracy: 0.7627 - val_loss: 0.5112\n",
            "Epoch 11/25\n",
            "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 9ms/step - accuracy: 0.8060 - loss: 0.4146 - val_accuracy: 0.7874 - val_loss: 0.4695\n",
            "Epoch 12/25\n",
            "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 9ms/step - accuracy: 0.8222 - loss: 0.3981 - val_accuracy: 0.7840 - val_loss: 0.4762\n",
            "Epoch 13/25\n",
            "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 9ms/step - accuracy: 0.8260 - loss: 0.3855 - val_accuracy: 0.7958 - val_loss: 0.4787\n",
            "Epoch 14/25\n",
            "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 9ms/step - accuracy: 0.8329 - loss: 0.3744 - val_accuracy: 0.7687 - val_loss: 0.5008\n",
            "Epoch 15/25\n",
            "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 9ms/step - accuracy: 0.8356 - loss: 0.3621 - val_accuracy: 0.7776 - val_loss: 0.5442\n",
            "Epoch 16/25\n",
            "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 9ms/step - accuracy: 0.8428 - loss: 0.3542 - val_accuracy: 0.7934 - val_loss: 0.4696\n",
            "Epoch 17/25\n",
            "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 9ms/step - accuracy: 0.8535 - loss: 0.3348 - val_accuracy: 0.8122 - val_loss: 0.4646\n",
            "Epoch 18/25\n",
            "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 9ms/step - accuracy: 0.8631 - loss: 0.3164 - val_accuracy: 0.8102 - val_loss: 0.4604\n",
            "Epoch 19/25\n",
            "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 9ms/step - accuracy: 0.8734 - loss: 0.3034 - val_accuracy: 0.8156 - val_loss: 0.4513\n",
            "Epoch 20/25\n",
            "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 9ms/step - accuracy: 0.8750 - loss: 0.2928 - val_accuracy: 0.7983 - val_loss: 0.5090\n",
            "Epoch 21/25\n",
            "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 9ms/step - accuracy: 0.8877 - loss: 0.2683 - val_accuracy: 0.7830 - val_loss: 0.5674\n",
            "Epoch 22/25\n",
            "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 9ms/step - accuracy: 0.8844 - loss: 0.2749 - val_accuracy: 0.7944 - val_loss: 0.4795\n",
            "Epoch 23/25\n",
            "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 9ms/step - accuracy: 0.8929 - loss: 0.2576 - val_accuracy: 0.7840 - val_loss: 0.5512\n",
            "Epoch 24/25\n",
            "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 8ms/step - accuracy: 0.8972 - loss: 0.2442 - val_accuracy: 0.8097 - val_loss: 0.4970\n",
            "Epoch 25/25\n",
            "\u001b[1m8000/8000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 9ms/step - accuracy: 0.9068 - loss: 0.2291 - val_accuracy: 0.7949 - val_loss: 0.5503\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7cf53190f6e0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**So before we fit our images to the neural network, we need to perform some image augmentations on them, which is basically synthesising the training data. We are going to do this using keras.preprocessing library for doing the synthesising part as well as to prepare the training set as well as the test test set of images that are present in a properly structured directories, where the directory’s name is take as the label of all the images present in it. For example : All the images inside the ‘cats’ named folder will be considered as cats by keras.**"
      ],
      "metadata": {
        "id": "O4tujAegc_u6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.save(\"/content/drive/MyDrive/Datasets_of_ML/cat_dog/saved_model_cat_dog1.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMeHoP0BgodR",
        "outputId": "cf617ef4-4852-4122-c890-01e4c907aef1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Part 3 - Making new predictions (uses Drive single_prediction or allows upload)\n",
        "# -----------------------------\n",
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "\n",
        "default_test_image = os.path.join(single_pred_folder, \"image.jpg\")\n",
        "\n",
        "print(\"Using test image:\", default_test_image)\n",
        "\n",
        "if not os.path.isfile(default_test_image):\n",
        "    print(\"Test image not found. Cannot run prediction.\")\n",
        "else:\n",
        "    try:\n",
        "        test_image = image.load_img(default_test_image, target_size=(64, 64))\n",
        "        test_image = image.img_to_array(test_image)\n",
        "        test_image = test_image / 255.0\n",
        "        test_image = np.expand_dims(test_image, axis=0)\n",
        "\n",
        "        result = classifier.predict(test_image)\n",
        "        prob = float(result[0][0])\n",
        "\n",
        "        if prob >= 0.6:\n",
        "            prediction = 'dog'\n",
        "        elif prob <= 0.4:\n",
        "            prediction = 'cat'\n",
        "        else:\n",
        "            prediction = 'unknown / not a cat or dog'\n",
        "\n",
        "        print(\"Prediction:\", prediction)\n",
        "        print(f\"Raw probability: {prob:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error while loading or predicting the image:\", str(e))"
      ],
      "metadata": {
        "id": "k-B2HhEUTsIf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55483a4d-111a-4e6f-be82-0012b6730454"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using test image: /content/drive/MyDrive/Datasets_of_ML/cat_dog/single_prediction/image.jpg\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
            "Prediction: cat\n",
            "Raw probability: 0.1246\n"
          ]
        }
      ]
    }
  ]
}